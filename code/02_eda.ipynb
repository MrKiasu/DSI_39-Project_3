{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff58b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854c0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f38b6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Post Text</th>\n",
       "      <th>ID</th>\n",
       "      <th>Score</th>\n",
       "      <th>Total Comments</th>\n",
       "      <th>Post URL</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>39</td>\n",
       "      <td>Sometimes you are anxious because the relation...</td>\n",
       "      <td>I‚Äôve been lurking in this subreddit for awhile...</td>\n",
       "      <td>zpw1zt</td>\n",
       "      <td>257</td>\n",
       "      <td>34</td>\n",
       "      <td>https://www.reddit.com/r/AnxiousAttachment/com...</td>\n",
       "      <td>AnxiousAttachment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>306</td>\n",
       "      <td>I daydream about getting hurt and then comforted.</td>\n",
       "      <td>How is this connected to anxious attachment st...</td>\n",
       "      <td>10kdtme</td>\n",
       "      <td>76</td>\n",
       "      <td>24</td>\n",
       "      <td>https://www.reddit.com/r/AnxiousAttachment/com...</td>\n",
       "      <td>AnxiousAttachment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>399</td>\n",
       "      <td>Feeling secure for the first time {FA}</td>\n",
       "      <td>I know a lot of people have followed my story,...</td>\n",
       "      <td>ujnfqw</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>https://www.reddit.com/r/AvoidantAttachment/co...</td>\n",
       "      <td>AvoidantAttachment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>533</td>\n",
       "      <td>Dating an AA and tables have turned. Now i kno...</td>\n",
       "      <td>I‚Äôm starting to gain some sympathy for the guy...</td>\n",
       "      <td>yi238v</td>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>https://www.reddit.com/r/AnxiousAttachment/com...</td>\n",
       "      <td>AnxiousAttachment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>340</td>\n",
       "      <td>DAs just don‚Äôt like us that much</td>\n",
       "      <td>I talked with my therapist a few days ago and ...</td>\n",
       "      <td>11gd4da</td>\n",
       "      <td>69</td>\n",
       "      <td>28</td>\n",
       "      <td>https://www.reddit.com/r/AnxiousAttachment/com...</td>\n",
       "      <td>AnxiousAttachment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>810</td>\n",
       "      <td>Healed avoidants, do you become attracted to m...</td>\n",
       "      <td>One of the big things to sabotage my love life...</td>\n",
       "      <td>tint2w</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>https://www.reddit.com/r/AvoidantAttachment/co...</td>\n",
       "      <td>AvoidantAttachment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>959</td>\n",
       "      <td>progress !! + uncovering my fears {fa}</td>\n",
       "      <td>today marks my 3-day streak of not deactivatin...</td>\n",
       "      <td>upzg3q</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>https://www.reddit.com/r/AvoidantAttachment/co...</td>\n",
       "      <td>AvoidantAttachment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>{DA} Vilification of avoidants and lack of tak...</td>\n",
       "      <td>This gets so exhausting. From popular resource...</td>\n",
       "      <td>125lfa9</td>\n",
       "      <td>165</td>\n",
       "      <td>81</td>\n",
       "      <td>https://www.reddit.com/r/AvoidantAttachment/co...</td>\n",
       "      <td>AvoidantAttachment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>735</td>\n",
       "      <td>How does anyone deal with hypersensitivity to ...</td>\n",
       "      <td>How do y‚Äôall deal with the tendency to see emb...</td>\n",
       "      <td>sxwegz</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.reddit.com/r/AvoidantAttachment/co...</td>\n",
       "      <td>AvoidantAttachment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>789</td>\n",
       "      <td>Apparently I‚Äôm just incapable of not smotherin...</td>\n",
       "      <td>Feeling frustrated, you guys. I long for conne...</td>\n",
       "      <td>whevsn</td>\n",
       "      <td>40</td>\n",
       "      <td>51</td>\n",
       "      <td>https://www.reddit.com/r/AnxiousAttachment/com...</td>\n",
       "      <td>AnxiousAttachment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              Title  \\\n",
       "1039          39  Sometimes you are anxious because the relation...   \n",
       "1306         306  I daydream about getting hurt and then comforted.   \n",
       "399          399             Feeling secure for the first time {FA}   \n",
       "1533         533  Dating an AA and tables have turned. Now i kno...   \n",
       "1340         340                   DAs just don‚Äôt like us that much   \n",
       "810          810  Healed avoidants, do you become attracted to m...   \n",
       "959          959             progress !! + uncovering my fears {fa}   \n",
       "30            30  {DA} Vilification of avoidants and lack of tak...   \n",
       "735          735  How does anyone deal with hypersensitivity to ...   \n",
       "1789         789  Apparently I‚Äôm just incapable of not smotherin...   \n",
       "\n",
       "                                              Post Text       ID  Score  \\\n",
       "1039  I‚Äôve been lurking in this subreddit for awhile...   zpw1zt    257   \n",
       "1306  How is this connected to anxious attachment st...  10kdtme     76   \n",
       "399   I know a lot of people have followed my story,...   ujnfqw     33   \n",
       "1533  I‚Äôm starting to gain some sympathy for the guy...   yi238v     52   \n",
       "1340  I talked with my therapist a few days ago and ...  11gd4da     69   \n",
       "810   One of the big things to sabotage my love life...   tint2w     18   \n",
       "959   today marks my 3-day streak of not deactivatin...   upzg3q     16   \n",
       "30    This gets so exhausting. From popular resource...  125lfa9    165   \n",
       "735   How do y‚Äôall deal with the tendency to see emb...   sxwegz     21   \n",
       "1789  Feeling frustrated, you guys. I long for conne...   whevsn     40   \n",
       "\n",
       "      Total Comments                                           Post URL  \\\n",
       "1039              34  https://www.reddit.com/r/AnxiousAttachment/com...   \n",
       "1306              24  https://www.reddit.com/r/AnxiousAttachment/com...   \n",
       "399               14  https://www.reddit.com/r/AvoidantAttachment/co...   \n",
       "1533              11  https://www.reddit.com/r/AnxiousAttachment/com...   \n",
       "1340              28  https://www.reddit.com/r/AnxiousAttachment/com...   \n",
       "810               15  https://www.reddit.com/r/AvoidantAttachment/co...   \n",
       "959               12  https://www.reddit.com/r/AvoidantAttachment/co...   \n",
       "30                81  https://www.reddit.com/r/AvoidantAttachment/co...   \n",
       "735                4  https://www.reddit.com/r/AvoidantAttachment/co...   \n",
       "1789              51  https://www.reddit.com/r/AnxiousAttachment/com...   \n",
       "\n",
       "               subreddit  class  \n",
       "1039   AnxiousAttachment      1  \n",
       "1306   AnxiousAttachment      1  \n",
       "399   AvoidantAttachment      0  \n",
       "1533   AnxiousAttachment      1  \n",
       "1340   AnxiousAttachment      1  \n",
       "810   AvoidantAttachment      0  \n",
       "959   AvoidantAttachment      0  \n",
       "30    AvoidantAttachment      0  \n",
       "735   AvoidantAttachment      0  \n",
       "1789   AnxiousAttachment      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41be890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "# Selecting both the title and post text as the text to be analysed.\n",
    "\n",
    "df[\"text\"] = df[\"Title\"] + ' ' + df[\"Post Text\"].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5fd0115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    1000\n",
       "1     998\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of rows for each class is similiar, i.e. no sign of class imbalance\n",
    "\n",
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc780fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>Dear Avoidant - I forgive you. And I forgive m...</td>\n",
       "      <td>dear avoidant  i forgive you and i forgive me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>‚ù§Ô∏è</td>\n",
       "      <td>‚ù§Ô∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Anyone struggle to be a kind and warm person i...</td>\n",
       "      <td>anyone struggle to be a kind and warm person i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>‚ù§Ô∏è‚Äçü©π {DA} {FA}</td>\n",
       "      <td>‚ù§Ô∏è‚Äçü©π da fa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>{fa} Do any other FAs also experience ‚Äòenmeshm...</td>\n",
       "      <td>fa do any other fas also experience ‚Äòenmeshmen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1168  Dear Avoidant - I forgive you. And I forgive m...   \n",
       "5                                                   ‚ù§Ô∏è    \n",
       "994   Anyone struggle to be a kind and warm person i...   \n",
       "47                                      ‚ù§Ô∏è‚Äçü©π {DA} {FA}    \n",
       "679   {fa} Do any other FAs also experience ‚Äòenmeshm...   \n",
       "\n",
       "                                             text_clean  \n",
       "1168  dear avoidant  i forgive you and i forgive me ...  \n",
       "5                                                   ‚ù§Ô∏è   \n",
       "994   anyone struggle to be a kind and warm person i...  \n",
       "47                                          ‚ù§Ô∏è‚Äçü©π da fa   \n",
       "679   fa do any other fas also experience ‚Äòenmeshmen...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuations and standardise to lowercase\n",
    "\n",
    "def remove_punct(text):\n",
    "    # store character only if it is not a punctuation\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(lambda x: remove_punct(x.lower()))\n",
    "df[[\"text\", \"text_clean\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090fe91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>Finding balance with AA and making a decision ...</td>\n",
       "      <td>finding balance with aa and making a decision ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>{FA}{DA} running into an ex, epiphanies, closu...</td>\n",
       "      <td>fada running into an ex epiphanies closure as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>I overcame my biggest fear Im not looking for ...</td>\n",
       "      <td>i overcame my biggest fear im not looking for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>I successfully battled through some of the sti...</td>\n",
       "      <td>i successfully battled through some of the sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>When the DA comes back: a warning We broke up ...</td>\n",
       "      <td>when the da comes back a warning we broke up t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>{fa} For those that have made significant stri...</td>\n",
       "      <td>fa for those that have made significant stride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>New to Understanding My Avoidant Attachment {f...</td>\n",
       "      <td>new to understanding my avoidant attachment fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>Dating is hard | {SA} {DA} Tired and I have a ...</td>\n",
       "      <td>dating is hard  sa da tired and i have a big d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>Hell, is where we reside ü´†</td>\n",
       "      <td>hell is where we reside ü´†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>{da} Fear of being 'bad' in bed? I mostly iden...</td>\n",
       "      <td>da fear of being bad in bed i mostly identify ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1739  Finding balance with AA and making a decision ...   \n",
       "353   {FA}{DA} running into an ex, epiphanies, closu...   \n",
       "1109  I overcame my biggest fear Im not looking for ...   \n",
       "169   I successfully battled through some of the sti...   \n",
       "1177  When the DA comes back: a warning We broke up ...   \n",
       "373   {fa} For those that have made significant stri...   \n",
       "587   New to Understanding My Avoidant Attachment {f...   \n",
       "530   Dating is hard | {SA} {DA} Tired and I have a ...   \n",
       "1024                        Hell, is where we reside ü´†    \n",
       "793   {da} Fear of being 'bad' in bed? I mostly iden...   \n",
       "\n",
       "                                             text_clean  \n",
       "1739  finding balance with aa and making a decision ...  \n",
       "353   fada running into an ex epiphanies closure as ...  \n",
       "1109  i overcame my biggest fear im not looking for ...  \n",
       "169   i successfully battled through some of the sti...  \n",
       "1177  when the da comes back a warning we broke up t...  \n",
       "373   fa for those that have made significant stride...  \n",
       "587   new to understanding my avoidant attachment fa...  \n",
       "530   dating is hard  sa da tired and i have a big d...  \n",
       "1024                         hell is where we reside ü´†   \n",
       "793   da fear of being bad in bed i mostly identify ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove urls in the text\n",
    "\n",
    "def remove_url(text):\n",
    "    text_nourl = re.sub(r'\\S*http\\S*', '', text)\n",
    "    return text_nourl\n",
    "\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_url(x))\n",
    "\n",
    "df[[\"text\", \"text_clean\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a743e342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>{fa}+{fa}: Deactivation versus legitimate conc...</td>\n",
       "      <td>fafa deactivation versus legitimate concerns a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>Signs of a good therapist</td>\n",
       "      <td>signs of a good therapist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>Core Wounds, Self-Sabotage, and Vulnerability ...</td>\n",
       "      <td>core wounds selfsabotage and vulnerability  st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>I think we can all relate to this :D</td>\n",
       "      <td>i think we can all relate to this d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>Knowing what you want {da} Do any other DAs or...</td>\n",
       "      <td>knowing what you want da do any other das or f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>Book recommendation: Adult Children of Emotion...</td>\n",
       "      <td>book recommendation adult children of emotiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>{DA} so i finally did It. Now I am lost. So, \\...</td>\n",
       "      <td>da so i finally did it now i am lost so \\r\\n\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>{fa} facepalm I was cuddling with the person I...</td>\n",
       "      <td>fa facepalm i was cuddling with the person ive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>Healthy Relationship Timeline {FA} So in true ...</td>\n",
       "      <td>healthy relationship timeline fa so in true fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>Anyone else feel oddly soothed during this cra...</td>\n",
       "      <td>anyone else feel oddly soothed during this cra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "506   {fa}+{fa}: Deactivation versus legitimate conc...   \n",
       "1282                         Signs of a good therapist    \n",
       "866   Core Wounds, Self-Sabotage, and Vulnerability ...   \n",
       "1994              I think we can all relate to this :D    \n",
       "323   Knowing what you want {da} Do any other DAs or...   \n",
       "868   Book recommendation: Adult Children of Emotion...   \n",
       "711   {DA} so i finally did It. Now I am lost. So, \\...   \n",
       "106   {fa} facepalm I was cuddling with the person I...   \n",
       "811   Healthy Relationship Timeline {FA} So in true ...   \n",
       "1861  Anyone else feel oddly soothed during this cra...   \n",
       "\n",
       "                                             text_clean  \n",
       "506   fafa deactivation versus legitimate concerns a...  \n",
       "1282                         signs of a good therapist   \n",
       "866   core wounds selfsabotage and vulnerability  st...  \n",
       "1994               i think we can all relate to this d   \n",
       "323   knowing what you want da do any other das or f...  \n",
       "868   book recommendation adult children of emotiona...  \n",
       "711   da so i finally did it now i am lost so \\r\\n\\r...  \n",
       "106   fa facepalm i was cuddling with the person ive...  \n",
       "811   healthy relationship timeline fa so in true fa...  \n",
       "1861  anyone else feel oddly soothed during this cra...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove words that contain digit\n",
    "def remove_digit(text):\n",
    "    text_nodigit = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    return text_nodigit\n",
    "\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_digit(x))\n",
    "\n",
    "df[[\"text\", \"text_clean\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe0c3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>Pay close enough attention and you‚Äôll realize ...</td>\n",
       "      <td>[pay, close, enough, attention, and, you, ll, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>Lack of sex/intimacy triggering negative core ...</td>\n",
       "      <td>[lack, of, sexintimacy, triggering, negative, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>I hate him. I hate him so much for making me f...</td>\n",
       "      <td>[i, hate, him, i, hate, him, so, much, for, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>Unjoined all of the relationship subreddits ju...</td>\n",
       "      <td>[unjoined, all, of, the, relationship, subredd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>‚Äúyou deserve better‚Äù one of the most pathetic ...</td>\n",
       "      <td>[, you, deserve, better, one, of, the, most, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1841  Pay close enough attention and you‚Äôll realize ...   \n",
       "1808  Lack of sex/intimacy triggering negative core ...   \n",
       "1690  I hate him. I hate him so much for making me f...   \n",
       "1472  Unjoined all of the relationship subreddits ju...   \n",
       "1550  ‚Äúyou deserve better‚Äù one of the most pathetic ...   \n",
       "\n",
       "                                             text_clean  \n",
       "1841  [pay, close, enough, attention, and, you, ll, ...  \n",
       "1808  [lack, of, sexintimacy, triggering, negative, ...  \n",
       "1690  [i, hate, him, i, hate, him, so, much, for, ma...  \n",
       "1472  [unjoined, all, of, the, relationship, subredd...  \n",
       "1550  [, you, deserve, better, one, of, the, most, p...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize\n",
    "def tokenize(text):  \n",
    "    # /W matches any character that is neither alphanumeric nor underscoreb\n",
    "    # Add a + just in case there are 2 or more spaces between certain words\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: tokenize(x)) \n",
    "df[[\"text\", \"text_clean\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a604683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>Coworker at work has a crush on another cowork...</td>\n",
       "      <td>[coworker, work, crush, another, coworker, act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>my avoidant ex is making an effort to get back...</td>\n",
       "      <td>[avoidant, ex, making, effort, get, back, toge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Guys‚Ä¶. {FA}</td>\n",
       "      <td>[guys, fa, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>How to deal with jealousy properly? I‚Äôve had h...</td>\n",
       "      <td>[deal, jealousy, properly, huge, anxiety, flar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>I think one of the saddest part in life is tha...</td>\n",
       "      <td>[think, one, saddest, part, life, absolutely, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1659  Coworker at work has a crush on another cowork...   \n",
       "1567  my avoidant ex is making an effort to get back...   \n",
       "125                                        Guys‚Ä¶. {FA}    \n",
       "1870  How to deal with jealousy properly? I‚Äôve had h...   \n",
       "1215  I think one of the saddest part in life is tha...   \n",
       "\n",
       "                                             text_clean  \n",
       "1659  [coworker, work, crush, another, coworker, act...  \n",
       "1567  [avoidant, ex, making, effort, get, back, toge...  \n",
       "125                                        [guys, fa, ]  \n",
       "1870  [deal, jealousy, properly, huge, anxiety, flar...  \n",
       "1215  [think, one, saddest, part, life, absolutely, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of default stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Remove stop words\n",
    "def remove_stopwords(tokenized_list):\n",
    "    #Store in text only if word is not found in stopword i.e. it is not a stopword\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_stopwords(x))\n",
    "df[[\"text\", \"text_clean\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc24d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>he reassured me i wouldnt be rejected and/or a...</td>\n",
       "      <td>reassured wouldnt rejected andor abandoned rej...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>Dismissive but also secure? Is that a thing? {...</td>\n",
       "      <td>dismissive also secure thing dasa im writing w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>Does anyone else start to feel resentment towa...</td>\n",
       "      <td>anyone else start feel resentment towards part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>Update: Today is better u/UpcycledThrowayAccnt...</td>\n",
       "      <td>update today better uupcycledthrowayaccnt aske...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>What DA \"fear of intimacy\" and \"deactivating s...</td>\n",
       "      <td>da fear intimacy deactivating strategy actuall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1878  he reassured me i wouldnt be rejected and/or a...   \n",
       "521   Dismissive but also secure? Is that a thing? {...   \n",
       "1910  Does anyone else start to feel resentment towa...   \n",
       "815   Update: Today is better u/UpcycledThrowayAccnt...   \n",
       "598   What DA \"fear of intimacy\" and \"deactivating s...   \n",
       "\n",
       "                                             text_clean  \n",
       "1878  reassured wouldnt rejected andor abandoned rej...  \n",
       "521   dismissive also secure thing dasa im writing w...  \n",
       "1910  anyone else start feel resentment towards part...  \n",
       "815   update today better uupcycledthrowayaccnt aske...  \n",
       "598   da fear intimacy deactivating strategy actuall...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatizer\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(tokenized_text):\n",
    "    #return list of all lemmatized words for their corresponding words in tokenized_text\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return ' '.join(text)\n",
    "\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: lemmatizing(x))\n",
    "df[[\"text\", \"text_clean\"]].sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd4b8c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Duplicate Words  Count (Class 0)  Count (Class 1)  Count Difference\n",
      "1210        lukewarm                1                1                 0\n",
      "3876     translation                1                1                 0\n",
      "1860            avid                1                1                 0\n",
      "4815          taxing                2                2                 0\n",
      "3877        insanity                1                1                 0\n",
      "...              ...              ...              ...               ...\n",
      "3140         anxious              218              685               467\n",
      "3336            like             1791             1322               469\n",
      "3257            feel             1879             1310               569\n",
      "3220              im             1495              855               640\n",
      "5119              fa              743               81               662\n",
      "\n",
      "[5302 rows x 4 columns]\n",
      "(5302, 4)\n"
     ]
    }
   ],
   "source": [
    "# Separate the text into two lists based on class\n",
    "class_0_text = df[df['class'] == 0][\"text_clean\"].tolist()\n",
    "class_1_text = df[df['class'] == 1][\"text_clean\"].tolist()\n",
    "\n",
    "# Function to find and count duplicate words between both classes\n",
    "def find_duplicate_words_between_classes(df, column_name, class_column_name):\n",
    "    class_0_text = df[df[class_column_name] == 0][column_name].tolist()\n",
    "    class_1_text = df[df[class_column_name] == 1][column_name].tolist()\n",
    "    \n",
    "    all_words_class_0 = ' '.join(class_0_text).split()\n",
    "    all_words_class_1 = ' '.join(class_1_text).split()\n",
    "    \n",
    "    common_words = set(all_words_class_0).intersection(all_words_class_1)\n",
    "    \n",
    "    word_counts_class_0 = Counter(all_words_class_0)\n",
    "    word_counts_class_1 = Counter(all_words_class_1)\n",
    "    \n",
    "    top_common_words_class_0 = {word: word_counts_class_0[word] for word in common_words if word in word_counts_class_0}\n",
    "    top_common_words_class_1 = {word: word_counts_class_1[word] for word in common_words if word in word_counts_class_1}\n",
    "    \n",
    "    return pd.DataFrame({'Duplicate Words': list(common_words), \n",
    "                         'Count (Class 0)': [top_common_words_class_0.get(word, 0) for word in common_words],\n",
    "                         'Count (Class 1)': [top_common_words_class_1.get(word, 0) for word in common_words]})\n",
    "\n",
    "# Get the DataFrame containing duplicate words and counts for both classes\n",
    "duplicate_words_df = find_duplicate_words_between_classes(df, \"text_clean\", \"class\")\n",
    "\n",
    "# Calculate the difference in counts between Class 0 and Class 1 for each word\n",
    "duplicate_words_df['Count Difference'] = (duplicate_words_df['Count (Class 0)'] - duplicate_words_df['Count (Class 1)']).abs()\n",
    "\n",
    "# Sort the DataFrame by the largest difference in counts (descending order)\n",
    "duplicate_words_df = duplicate_words_df.sort_values(by='Count Difference', ascending=True)\n",
    "\n",
    "# Display the duplicate words and counts for both classes\n",
    "print(duplicate_words_df)\n",
    "print(duplicate_words_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dedff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the list of duplicate words whose frequency in class 0/1 differed by less than 10 as additional stopwords to further clean the corpus\n",
    "\n",
    "additional_stopwords = duplicate_words_df[duplicate_words_df['Count Difference'].between(0, 10)]\n",
    "stopword += additional_stopwords[\"Duplicate Words\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7b0433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_working\"] = df[\"text_clean\"].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b44309e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_working</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>dating middle age youre imagining thing</td>\n",
       "      <td>[n, g,  ,  , g,  , u, r,  , g, n, n, g,  , h, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>gotos selfsoothing im going pretty dark time t...</td>\n",
       "      <td>[g,  , h, n, g,  ,  , g, n, g,  , r,  , r,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>ive enmeshed someone hello folk ive written fr...</td>\n",
       "      <td>[v,  , n, h,  , n,  , h,  ,  , v,  , r, n,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>feel loved together constantly worry partner c...</td>\n",
       "      <td>[ , v,  , g, h, r,  , n, n,  , r, r,  , r, n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>right thing today honest wanting something ser...</td>\n",
       "      <td>[r, g, h,  , h, n, g,  ,  , h, n,  , n, n, g, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_clean  \\\n",
       "181            dating middle age youre imagining thing    \n",
       "1532  gotos selfsoothing im going pretty dark time t...   \n",
       "344   ive enmeshed someone hello folk ive written fr...   \n",
       "1940  feel loved together constantly worry partner c...   \n",
       "1613  right thing today honest wanting something ser...   \n",
       "\n",
       "                                           text_working  \n",
       "181   [n, g,  ,  , g,  , u, r,  , g, n, n, g,  , h, ...  \n",
       "1532  [g,  , h, n, g,  ,  , g, n, g,  , r,  , r,  , ...  \n",
       "344   [v,  , n, h,  , n,  , h,  ,  , v,  , r, n,  , ...  \n",
       "1940  [ , v,  , g, h, r,  , n, n,  , r, r,  , r, n, ...  \n",
       "1613  [r, g, h,  , h, n, g,  ,  , h, n,  , n, n, g, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"text_clean\", \"text_working\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3997efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "def remove_stopwords(tokenized_list):\n",
    "    #Store in text only if word is not found in stopword i.e. it is not a stopword\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "df[\"text_working\"] = df[\"text_clean\"].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7037705d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_working</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>got text feel sad ashamed know posting usually...</td>\n",
       "      <td>[g,  ,  ,  ,  , h,  , n,  , n, g,  , u, u,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>put together list youre looking yet make sure ...</td>\n",
       "      <td>[u,  , g, h, r,  ,  , u, r,  , n, g,  ,  ,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>envy secure attachers envy fact deal partner w...</td>\n",
       "      <td>[n, v,  , u, r,  , h, r,  , n, v,  ,  ,  , r, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>putting disturb best thing ever done ap worry ...</td>\n",
       "      <td>[u, n, g,  , u, r,  ,  , h, n, g,  , v, r,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>fa stop serious tense learn find balance life ...</td>\n",
       "      <td>[ ,  , r, u,  , n,  , r, n,  , n,  , n,  ,  , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_clean  \\\n",
       "1286  got text feel sad ashamed know posting usually...   \n",
       "1167  put together list youre looking yet make sure ...   \n",
       "1818  envy secure attachers envy fact deal partner w...   \n",
       "1384  putting disturb best thing ever done ap worry ...   \n",
       "942   fa stop serious tense learn find balance life ...   \n",
       "\n",
       "                                           text_working  \n",
       "1286  [g,  ,  ,  ,  , h,  , n,  , n, g,  , u, u,  , ...  \n",
       "1167  [u,  , g, h, r,  ,  , u, r,  , n, g,  ,  ,  , ...  \n",
       "1818  [n, v,  , u, r,  , h, r,  , n, v,  ,  ,  , r, ...  \n",
       "1384  [u, n, g,  , u, r,  ,  , h, n, g,  , v, r,  , ...  \n",
       "942   [ ,  , r, u,  , n,  , r, n,  , n,  , n,  ,  , ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"text_clean\", \"text_working\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043b465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
